{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNTJ3bvrp/rvEEOrBml3qMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagaliDrumare/Copilot-Azure-Open-AI-Microsoft/blob/main/Azure_Open_AI_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDR6LsAcLBuf"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.13.3 -qqq\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import os\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = AzureOpenAI(\n",
        "    azure_endpoint = \"https://pleinelune.openai.azure.com/openai/deployments/gpt-35-turbo-16k/chat/completions?api-version=2023-03-15-preview\",  # Replace with your Azure OpenAI endpoint\n",
        "    api_key = \"df0cdc00c0194fb69fae1226a5f29bc3\",  # Replace with your API key\n",
        "    api_version = \"2024-02-15-preview\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "M7gQjK5JLHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How can AI impact education over the next decade.\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-35-turbo-16k\", # model = \"deployment_name\"\n",
        "  messages = message,\n",
        "  temperature=0.7,\n",
        "  max_tokens=800,\n",
        "  top_p=0.95,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=None\n",
        ")"
      ],
      "metadata": {
        "id": "NsR33_3GLOgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from datetime import datetime  # Make sure to include this import\n",
        "\n",
        "def print_formatted_response(completion):\n",
        "    # Print a header to separate each response\n",
        "    print(\"\\n--- Response Details ---\")\n",
        "\n",
        "    # Loop through each choice in the completion (typically there will be just one)\n",
        "    for choice in completion.choices:\n",
        "        # Print the response content\n",
        "        print(\"\\nContent:\")\n",
        "        print(choice.message.content)\n",
        "\n",
        "        # Print the finish reason\n",
        "        print(\"\\nFinish Reason:\", choice.finish_reason)\n",
        "\n",
        "        # Print the role of the message\n",
        "        print(\"\\nRole:\", choice.message.role)\n",
        "\n",
        "        # Print tool calls and function call if applicable\n",
        "        print(\"\\nFunction Call:\", choice.message.function_call)\n",
        "        if choice.message.tool_calls:\n",
        "            print(\"\\nTool Calls:\")\n",
        "            for tool_call in choice.message.tool_calls:\n",
        "                print(tool_call)\n",
        "\n",
        "        # Print content filter results if available\n",
        "        if hasattr(choice, 'content_filter_results'):  # Check if content filter results exist\n",
        "            print(\"\\nContent Filter Results:\")\n",
        "            for key, value in choice.content_filter_results.items():\n",
        "                filtered_status = \"Filtered=\" + str(value.get('filtered', 'N/A'))  # Handle missing 'filtered' key\n",
        "                severity_level = \"Severity=\" + str(value.get('severity', 'N/A'))  # Handle missing 'severity' key\n",
        "                print(f\"{key.capitalize()}: {filtered_status}, {severity_level}\")\n",
        "        else:\n",
        "            print(\"\\nContent Filter Results: Not Available\")  # Inform if content filtering is not supported\n",
        "\n",
        "    # Print token usage information\n",
        "    print(\"\\nTokens Used:\", completion.usage.total_tokens)\n",
        "\n",
        "    # Print the model used\n",
        "    print(\"\\nModel Used:\", completion.model)\n",
        "\n",
        "    # Print the creation time\n",
        "    print(\"\\nCreated At:\", completion.created)\n",
        "\n",
        "    # Convert Unix timestamp to human-readable date and time\n",
        "    created_at = datetime.utcfromtimestamp(completion.created).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "    print(\"\\nCreated At:\", created_at)\n",
        "\n",
        "# Print formatted response\n",
        "print_formatted_response(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mukdmeL0OleV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}